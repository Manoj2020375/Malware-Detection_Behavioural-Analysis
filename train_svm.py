import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score
import random
from scipy import stats
from matplotlib import pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras import initializers
from keras.callbacks import EarlyStopping
import tensorflow as tf
from keras.models import load_model
from keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras import layers, losses
from rl.data import get_train_test_val, load_csv
from sklearn.metrics import confusion_matrix
from sklearn.metrics import (f1_score, accuracy_score, recall_score, precision_score)
from sklearn.svm import SVC



# Dropping categorical columns and columns that have the same value for all rows.
X_train, y_train, X_test, y_test = load_csv("./data/network0.csv", "./data/network1.csv", "goal_encoded", ["DestIP", "Dport", "goal", "hash", "url", "url_query_names", "url_query_values", "path", "filename", "hostname", "sni", "cert_subject", "cert_issuer", "Src_P", "Dest_IP", "Dest_P", "SNI_equal_DstIP", "filename_caseratio", "hostname_caseratio", "sni_caseratio", "dns_nxdomain_caseratio", "dns_success_caseratio"], normalization=True)
X_train, y_train, X_test, y_test, X_val, y_val = get_train_test_val(X_train, y_train, X_test, y_test,
                                                                    val_frac=0.3)


encoder = OneHotEncoder()
encoded_y_train = encoder.fit_transform(y_train.reshape(-1, 1))
encoded_y_val = encoder.transform(y_val.reshape(-1, 1))
encoded_y_test = encoder.transform(y_test.reshape(-1, 1))

# Build the model
model = Sequential() # create model
model.add(Dense(256, input_shape=X_train.shape[1:], trainable=True,activation='relu')) # hidden
model.add(Dense(256, trainable=True,activation='relu')) # hidden
model.add(Dense(256, trainable=True,activation='relu')) # hidden 
model.add(Dense(9, trainable=True, activation='softmax')) # output layer


#define the optimizer
adam = tf.keras.optimizers.Adam(learning_rate=0.0005)

# Compile the model.
model.compile(
  optimizer=adam,
  loss='categorical_crossentropy',
  metrics=['accuracy'],
)

# obtain the summary
# model.summary()

#  early stopping
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 5)
mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', 
                     verbose=1, save_best_only=True)

# Train the model.
history=model.fit(
  X_train,
  to_categorical(y_train),
  validation_data=(X_val, to_categorical(y_val)),
  epochs=3000,
  batch_size=512,
  shuffle = True,
  callbacks=[es,mc],
)

import json
with open('history_nn.json', 'w') as f:
    json.dump(list(map(str, history.history['loss'])), f)

y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)
matrix = confusion_matrix(y_test, y_pred)
print("matrix:\n", matrix)
classwise_accuracy = matrix.diagonal()/matrix.sum(axis=1)
print('Accuracy:', accuracy)
print('Classwise Accuracy:', classwise_accuracy)


micro_precision = precision_score(y_test, y_pred, average='micro', zero_division=0)
micro_recall = recall_score(y_test, y_pred, average='micro')
micro_f1 = f1_score(y_test, y_pred, average='micro')

weighted_precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
weighted_recall = recall_score(y_test, y_pred, average='weighted')
weighted_f1 = f1_score(y_test, y_pred, average='weighted')
print('Micro Precision:', micro_precision, 'Weighted Precision:', weighted_precision)
print('Micro Recall:', micro_recall, 'Weighted Recall:', weighted_recall)
print('Micro F1 score:', micro_f1, 'Weighted F1 score:', weighted_f1)